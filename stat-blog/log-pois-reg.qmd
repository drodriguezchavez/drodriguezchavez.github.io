---
title: "Logistic and Poisson Regression"
date: 10/14/2025
Keywords: "regression, logistic, poisson, maximum likelihood, Bayes Theorem"
---

### Bayes' Theorem

Bayes' theorem is a theorem of conditional probability formulated using other probability definitions.

Given event *A* and event *B*, the probability of *A* and *P* can be re-written as a conditional probability.

$$
P(A \hspace{1mm} and \hspace{1mm} B) = P(A|B)P(B) = P(B|A)P(A)
$$

We can thus set these two conditional probabilities equal to each other and get that

$$
 P(A|B) = \frac{P(B|A)P(A)}{P(B)}
$$ Bayes' theorem has been revolutionary in the field of statistics. Given evidence (or data) *X* and parameters $\theta$, the above theorem is often written in the form

$$
P(\theta|X) = \frac{P(X|\theta)P(\theta)}{P(X)}
$$ Where $P(\theta|X)$ can be viewed as the probability of the parameters given the data. In Bayesian statistics, it is also known as the *posterior distribution.* $P(X|\theta)$ is known as the *likelihood* (the probability of the data given the parameters) and $P(\theta)$ is known as the *prior.* $P(X)$ is often known as the normalizing constant and thanks to the total law of probability, can be written as $\int P(X|\theta)P(\theta)d\theta$. However, this is often very hard to find, and is also a constant, so often one will see that

$$
P(\theta|X) \propto P(X|\theta)P(\theta)
$$

Where $\propto$ means \`\`proportional to.'' Usually, $P(\theta)$ will be set beforehand based on prior knowledge. Or it will be set to a neutral condition to signifiy no knowledge. And then $P(X|\theta)$ can be calculated using maximum likelihood, which is where we use the condition of independence to calculate the most likely parameter $\theta$. See my blog post on maximum likelihood [here](https://drodriguezchavez.github.io/stat-blog/maximum-likelihood.html).

### When do we use Bayes' Theorem?
 
### Logistic Regression
Logistic regression a way of modeling a binary outcome (i.e. an event happening or not). 

$$
Y_i | X_i \sim Binomial(\pi)
$$
The covariates change the log-odds of an event. 

In linear regression, the slope had an additive effect, in logistic regression, the effect is multiplicative. if $\beta_1 < 0$, it is a protective factor. Decreases the odds of the event happening. If $\beta_1 > 0$, it is a risk factor and increases the odds of the event happening. 

$$
log(\frac{\pi}{1-\pi}) = \beta_0 + \beta_1 X
$$

```{r}
prob.pi <- seq(0, 1, length.out = 200)
prob.pi.odds <- prob.pi/ (1-prob.pi)
prob.pi.logodds <- log(prob.pi.odds)

plot(prob.pi, rep(0, 200), col = 'blue', pch = 19, cex = 0.3, ylim = c(-8, 15)) + 
  points(prob.pi, prob.pi.odds, col = 'green', pch = 19, cex = 0.3) +
  points(prob.pi, prob.pi.logodds, col = 'red', pch = 19, cex = 0.3) 
```

How do you estimate these parameters? 
Maximum likelihood estimation!

```{r}
vaping <- c(rep(0, 20), rep(1, 20))
logodds <- 0.5 + 0.5 * vaping
prob.pi <- 1 / (1 + exp(-logodds))
Y <- rbinom(40, size = 1, prob = prob.pi)

fit <- glm(Y ~ vaping, family = binomial)
summary(fit)
```


### Poisson Regression

$$
Y \sim Poisson(\lambda) \\\\
log(\lambda) = \beta_0 + \beta_1 X
$$

### Additional Resources

- Very Normal on Logistic Regression: https://www.youtube.com/watch?v=Iju8l2qgaJU 
---
title: "Simple Linear Regression and Hypothesis Testing"
date: 09/24/2025
Keywords: ""
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

### What is the point of linear regression?

Given an experiment with one independent variable and one dependent variable, linear regression is a way to estimate how much the dependent variable relates to the independent variable. Let's write out a simple linear regression below with an intercept and one covariate ($X_i$).

$$
 Y_i = \beta_0 + \beta_1 X_i + \epsilon_i , \epsilon_i \sim N(0, \sigma^2)
$$ The four model assumptions that we have with linear regression are...

1.  **Linearity**. The dependent variable and independent variable have a linear relationship.

2.  **Homoskedasticity**. This means that the variance of the errors is the same for each data point, $i$.

3.  **Normality**. The errors are Normally distributed.

4.  **Independence**. This assumes that each data vector $i$ is independent from another data vector, $j$. In other words, the correlation($\epsilon_i$, $\epsilon_j$) = 0.

Let's explore a simulated set of data to better visualize this. Simulations are nice because we can make sure our models are working as we think they should before applying it to real data. For the next bit, we will be using $\beta_0 = 1$, $\beta_1 = 1$, $\sigma^2 = 20$, and with 100 data points. 

```{r, simulating a dataset}
# simulating our data
N <- 100
Xi <- runif(min = 1, max = 10, n = N)

# getting our parameters 
beta0 <- 1
beta1 <- 1
var <- 20

# putting it together 
e <- rnorm(N, mean = 0, sd = sqrt(var))
Yi <- beta0 + beta1*Xi + e 

# note that this formulation is equivalent to the above. 
# Yi <- rnorm(N, mean = beta0 + beta1*Xi, sd = sqrt(var)) 

plot(Xi,Yi, pch = 16) 
```

*What's our goal in linear regression?* We have an independent variable $X_i$, and we want to see how it relates to our dependent variable $Y_i$ in this linear relationship, of an intercept and a slope. The intercept, $\beta_0$ is the average value of our outcome, $Y_i$ when all of our independent variables, or predictors, are set to 0. Our slope, on the other hand, can be understood as the magnitude of the change for the dependent variable given a 1 unit increase of the predictor (independent) variable. 

So, our goal is to minimize a line through all data $(X_i, Y_i)$, given our choice of $\beta_0$ and $\beta_1$. One way of thinking about how to do it is to turn it into a minimization problem. We want to minimize the distance between the line (our model-predicted values in a sense) and the data itself. If $Y_i$ is our dependent  variable, then we can write $\hat{Y}_i = \beta_0 + \beta_1 * X_i$ as our model-predicted values (our line)! Thus, our goal is to minimize
$$ 
min \space \Sigma_{i=1}^n (Y_i - [\beta_0 + X_i \beta_1])^2 
$$
### Let's take a quick dive into matrices for a second. 
If we want to solve the least squares equations, it can be helpful to re-write our formula in terms of matrices. 
Let's imagine $Y_i$ as a vector, where each row $i$ containing one observation. Below, we can see the first 10 rows of the vector $Y_i$ from our simulation. This is our true data! 


```{r}
matrix(Yi[1:10])
```

Our goal is to know find a single value for $\beta_0$ and a single value for $\beta_1$ such that $\beta_0 + \beta_1 * X_i$ is our best fit line to $Y_i$. Note that because in this case, because we simulated the data, we technically know what $\beta_0$ and $\beta_1$ should be. But, in non-simulated data land, we won't know neither of these values, nor the extent of the variance $\sigma^2$. We will just have our best guess at a relationship. In this case it's a linear one: $\beta_0 + \beta_1 * X_i$.

Let's re-write this line in matrix form such that for row $i$, we get that $\hat{Y}_i = \beta_0 + \beta_1 * X_i$. (I put a hat on $Y$ to showcase that it is an estimate and not a true value. It can also be known as our expected value of Y).

Recalling our rules of matrices, we can re-write it as 
$$ 
E[Y] = \hat{Y} = X\vec{\beta}
$$
where
$$ 
\begin{align}
X &=  \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_n  \end{bmatrix}
\\\\
\vec{\beta} &= \begin{bmatrix} \beta_0 \\ \beta_1 \end{bmatrix}
\end{align}
$$

Note that $X$ in this context is called the design matrix! It is a $n \times p$ matrix, where $n$ is the number of observations and $p$ is the number of coefficients ($\beta$'s) to estimate. 

### Solving the least squares equation
Now we can rewrite the least squared equation (also known as the mean squared error) as 
$$ 
min (Y - X \vec{\beta})^2 = \hat{\epsilon}^2
$$
Note that we can re-write this as 
$$
\begin{align}
min (Y - X \vec{\beta})^2 & = min( (Y - X\vec{\beta})^T(Y - X\vec{\beta}) 
\\\\ &= min (Y^TY - Y^TX \vec{\beta} - \vec{\beta}^T X^T Y + \vec{\beta}^T X^T X \vec{\beta})
\\\\ &= min (Y^TY - 2\vec{\beta}^T X^T Y + \vec{\beta}^T X^T X \vec{\beta})
\end{align}
$$
Because we know what $X$ and $Y$ are already, our goal is to find the right $\beta$'s to minimize this equation. So we differentiate and set the equation equal to 0 to find an inflection point (either a min or a max.)

$$ 
\begin{align}
0 &= \frac{d}{d\beta} (Y^TY - 2\vec{\beta}^T X^T Y + \vec{\beta}^T X^T X \vec{\beta})
\\\\ &= - 2 X^T Y + 2 X^T X \vec{\beta}
\\\\ &=  - X^T Y + X^T X \vec{\beta}
\\\\ X^T Y &=  X^T X \vec{\beta}
\\\\ (X^T X)^{-1} X^T Y &=  \vec{\beta}
\end{align}
$$
Now we have our $\beta$'s that solve our least squares equation! 

### Going back to predictions and the hat matrix
Because we know that $(X^T X)^{-1} X^T Y =  \vec{\beta}$, we can put it back into our equation from before of $E[Y] = \hat{Y} = X\vec{\beta}$ to get that 
$$
E[Y] = \hat{Y} = X\vec{\beta} = X (X^T X)^{-1} X^T Y
$$
Where this matrix $X (X^T X)^{-1} X^T$ is known as the hat matrix (since it is a matrix that when applied to a vector of observed data, $Y$, it will give back the expected value given the covariates $X$, or the predicted values, which conventionally in notation puts a hat over the variable $\hat{Y}$).

Can we check this with our simulated data? 
```{r, fitting a line and comparing}
# doing linear regression in r
lm.fit <- lm(Yi ~ Xi)


coef(lm.fit)

# deriving it using the hat matrix
X.matrix <- as.data.frame(Xi) |>
  # add_column(X2 = X2) |>
  add_column(int = 1, .before = 1) |>
  as.matrix()

# we should get back the same thing estimates from the lm 
betas <- solve(t(X.matrix)%*%X.matrix)%*%t(X.matrix)%*%as.matrix(Yi)
betas

```


### Wait, but what about our variance? 


```{r, looking at the variance}
# H <- X.matrix%*%solve(t(X.matrix)%*%X.matrix)%*%t(X.matrix)
# e.dot <- Y1 - H%*%as.matrix(Y1)
# sigma.hat <- (t(e.dot)%*%e.dot)/(N-2) # just a vector of sigmas
# sigma.hat <- matrix(c(sigma.hat,0,0,sigma.hat), nrow = 2) # sigma*Identity matrix 
# C <- solve(t(X.matrix)%*%X.matrix)
# 
# # note that the residual standard error from lm should match these diagonals  
# sqrt(sigma.hat)
# 
# # these should also match. ADD A DESCRIPTION AS TO WHY 
# varcovar.matrix <- sigma.hat%*%C
# varcovar.matrix
# vcov(lm.fit) # compare with above 
# 
# # we should get the same estimates of the SE from the lm 
# sqrt(varcovar.matrix[1,1])
# sqrt(varcovar.matrix[2,2])
```
Confidence intervals, p-values, linear least squares.

Simulations

How do we estimate

How does the Central Limit Theorem tie into this?

How are test statistics and confidence intervals related?

R\^2

### Looking at an example in R

### For future

-   Bernoulli/Binomial distributions, Poisson distributions, interactions

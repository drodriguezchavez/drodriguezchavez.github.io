---
title: "Simple Linear Regression and Hypothesis Testing"
date: 09/24/2025
Keywords: ""
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

### What is the point of linear regression?

Given an experiment with one independent variable and one dependent variable, linear regression is a way to estimate how much the dependent variable relates to the independent variable. Let's write out a simple linear regression below with an intercept and one covariate ($X_i$).

$$
 Y_i = \beta_0 + \beta_1 X_i + \epsilon_i , \epsilon_i \sim N(0, \sigma^2)
$$ The four model assumptions that we have with linear regression are...

1.  **Linearity**. The dependent variable and independent variable have a linear relationship.

2.  **Homoskedasticity**. This means that the variance of the errors is the same for each data point, $i$.

3.  **Normality**. The errors are Normally distributed.

4.  **Independence**. This assumes that each data vector $i$ is independent from another data vector, $j$. In other words, the correlation($\epsilon_i$, $\epsilon_j$) = 0.

Let's explore a simulated set of data to better visualize this. Simulations are nice because we can make sure our models are working as we think they should before applying it to real data. For the next bit, we will be using $\beta_0 = 1$, $\beta_1 = 1$, $\sigma^2 = 20$, and with 100 data points.

```{r, simulating a dataset}
# simulating our data
N <- 100
Xi <- runif(min = 1, max = 10, n = N)

# getting our parameters 
beta0 <- 1
beta1 <- 1
var <- 20

# putting it together 
e <- rnorm(N, mean = 0, sd = sqrt(var))
Yi <- beta0 + beta1*Xi + e 

# note that this formulation is equivalent to the above. 
# Yi <- rnorm(N, mean = beta0 + beta1*Xi, sd = sqrt(var)) 

plot(Xi,Yi, pch = 16) 
```

**What's our goal in linear regression?** We have an independent variable $X_i$, and we want to see how it relates to our dependent variable $Y_i$ in this linear relationship, of an intercept and a slope. The intercept, $\beta_0$ is the average value of our outcome, $Y_i$ when all of our independent variables, or predictors, are set to 0. Our slope, on the other hand, can be understood as the magnitude of the change for the dependent variable given a 1 unit increase of the predictor (independent) variable.

So, our goal is to minimize a line through all data $(X_i, Y_i)$, given our choice of $\beta_0$ and $\beta_1$. One way of thinking about how to do it is to turn it into a minimization problem. We want to minimize the distance between the line (our model-predicted values in a sense) and the data itself. If $Y_i$ is our dependent variable, then we can write $\hat{Y}_i = \beta_0 + \beta_1 * X_i$ as our model-predicted values (our line)! Thus, our goal is to minimize 
$$ 
min \space \Sigma_{i=1}^n (Y_i - [\beta_0 + X_i \beta_1])^2 
$$

### Let's take a quick dive into matrices for a second. 

If we want to solve the least squares equations, it can be helpful to re-write our formula in terms of matrices. Let's imagine $Y_i$ as a vector, where each row $i$ containing one observation. Below, we can see the first 10 rows of the vector $Y_i$ from our simulation. This is our true data!

```{r}
matrix(Yi[1:10])
```

Our goal is to know find a single value for $\beta_0$ and a single value for $\beta_1$ such that $\beta_0 + \beta_1 * X_i$ is our best fit line to $Y_i$. Note that because in this case, because we simulated the data, we technically know what $\beta_0$ and $\beta_1$ should be. But, in non-simulated data land, we won't know neither of these values, nor the extent of the variance $\sigma^2$. We will just have our best guess at a relationship. In this case it's a linear one: $\beta_0 + \beta_1 * X_i$.

Let's re-write this line in matrix form such that for row $i$, we get that $\hat{Y}_i = \beta_0 + \beta_1 * X_i$. (I put a hat on $Y$ to showcase that it is an estimate and not a true value. It can also be known as our expected value of Y).

Recalling our rules of matrices, we can re-write it as 
$$ 
E[Y] = \hat{Y} = X\vec{\beta}
$$

where 
$$ 
\begin{align}
X &=  \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_n  \end{bmatrix}
\\\\
\vec{\beta} &= \begin{bmatrix} \beta_0 \\ \beta_1 \end{bmatrix}
\end{align}
$$

Note that $X$ in this context is called the design matrix! It is a $n \times p$ matrix, where $n$ is the number of observations and $p$ is the number of coefficients ($\beta$'s) to estimate.

### Solving the least squares equation

Now we can rewrite the least squared equation (also known as the mean squared error) as $$ 
min (Y - X \vec{\beta})^2 = \hat{\epsilon}^2
$$ Note that we can re-write this as 
$$
\begin{align}
min (Y - X \vec{\beta})^2 & = min( (Y - X\vec{\beta})^T(Y - X\vec{\beta}) 
\\\\ &= min (Y^TY - Y^TX \vec{\beta} - \vec{\beta}^T X^T Y + \vec{\beta}^T X^T X \vec{\beta})
\\\\ &= min (Y^TY - 2\vec{\beta}^T X^T Y + \vec{\beta}^T X^T X \vec{\beta})
\end{align}
$$ 
Because we know what $X$ and $Y$ are already, our goal is to find the right $\beta$'s to minimize this equation. So we differentiate and set the equation equal to 0 to find an inflection point (either a min or a max.)

$$ 
\begin{align}
0 &= \frac{d}{d\beta} (Y^TY - 2\vec{\beta}^T X^T Y + \vec{\beta}^T X^T X \vec{\beta})
\\\\ &= - 2 X^T Y + 2 X^T X \vec{\beta}
\\\\ &=  - X^T Y + X^T X \vec{\beta}
\\\\ X^T Y &=  X^T X \vec{\beta}
\\\\ (X^T X)^{-1} X^T Y &=  \vec{\beta}
\end{align}
$$ Now we have our $\beta$'s that solve our least squares equation!

### Going back to predictions and the hat matrix

Because we know that $(X^T X)^{-1} X^T Y =  \vec{\beta}$, we can put it back into our equation from before of $E[Y] = \hat{Y} = X\vec{\beta}$ to get that $$
E[Y] = \hat{Y} = X\vec{\beta} = X (X^T X)^{-1} X^T Y
$$ Where this matrix $X (X^T X)^{-1} X^T$ is known as the hat matrix (since it is a matrix that when applied to a vector of observed data, $Y$, it will give back the expected value given the covariates $X$, or the predicted values, which conventionally in notation puts a hat over the variable $\hat{Y}$).

**Can we check this with our simulated data?**

```{r, fitting a line and comparing}
# doing linear regression in r
lm.fit <- lm(Yi ~ Xi)
# getting the coefficients from the r output
coef(lm.fit)

# creating our design matrix
X.matrix <- as.data.frame(Xi) |>
  # adding the column of intercepts
  add_column(int = 1, .before = 1) |>
  as.matrix()

# solving for the betas from (X^T X)^{-1} X^T
betas <- solve(t(X.matrix)%*%X.matrix)%*%t(X.matrix)%*%as.matrix(Yi)
betas

# graphing our line 
plot(Xi,Yi, pch = 16) 
lines(Xi, X.matrix%*%betas)
```

Looks good! So to recap, we now have our beta estimates for our linear regression.

### Wait, but what about our variance?

Recall that our variance $\epsilon_i \sim N(0, \sigma^2)$. How do we estimate $\sigma^2$?

Adding on from the previous section, we have that our hat matrix is $H = X(X^T X)^{-1} X^T$. So we can rewrite the linear regression and then the errors as... 

$$ 
\begin{align}
Y_i &= \beta_0 + \beta_1 * X_i + \epsilon_i 
\\\\ Y &= X \vec{\beta} + \vec{\epsilon}
\\\\ &= HY + \vec{\epsilon}
\\\\ \vec{\epsilon} &= Y - HY
\\\\ &= (I- H)Y
\end{align}
$$ 
So now we have a way of getting the residuals from our hat matrix! However, we still need to find way to estimate $\sigma$. 

<!-- $$ -->
<!-- \begin{align} -->
<!-- Var(Y) &= Var(X\vec{\beta}+\vec{\epsilon}) -->
<!-- \\\\ &= Var(X\vec{\beta}) + Var(\vec{\epsilon}) + 2Cov(X\vec{\beta}, \vec{\epsilon}) -->
<!-- \\\\ &= Var(\vec{\epsilon}) -->
<!-- \\\\ &= Var[(I-H)Y]  -->
<!-- \\\\ &= (I-H)Var(Y)(I-H)^T -->
<!-- \\\\ &= (I-H)Var(\vec{\epsilon})(I-H)^T -->
<!-- \\\\ &= (I-H) \sigma^2 (I-H)^T -->
<!-- \\\\ &= \sigma^2 (I-H) -->
<!-- \end{align} -->
<!-- $$  -->

<!-- There were many jumps here. The first is just using properties of variance decomposition. Then we recall that the variance of true values (like $\beta$) are 0 and that we have made an assumption that $X$ is uncorrelated with $\epsilon$. Next, we note that there is a property that given a vector $v$ and a matrix $X$, $Var(Xv) = X*Var(v)*X^T$. Finally, the properties of the hat matrix are that $(I-H) = (I-H)^T$ (called being symmetric) and $H^TH = H$ so therefore $(I-H)^T(I-H) = I-H$. -->

An intuitive way I like to think the variance, is that it quantifies how dispersed the data is from the mean. In other words, how far $Y_i$ is from $\beta_0 + \beta_1 * X_i$ averaged over the whole data set. Sound familiar? This is the same idea as our mean squared error equation from before. This quantity is often called **sum of squared errors** or **residual sum of squares**. 

$$
\begin{align}
\sigma^2 \rightarrow s^2 &= \Sigma_{i=1}^{n} (Y_i - H*Y_i)^2 / (n-p)
\\\\ &= \vec{\epsilon}^2 / (n-p)
\\\\ &= \vec{\epsilon}^T\vec{\epsilon}/(n-p)
\end{align}
$$
We use $s^2$ to denote the estimated variance. Note that the bottom part of the above derivation is just how we would write in matrix form from the definition of the residuals that we showed earlier. 

Why do we divide by $n-p$ and not just $n$ to average over the whole set? This is because we just have $n-p$ degrees of freedom. An intuitive way to think about this is that each data point you have gives you information. And the more parameters you estimate, the more you use up these information data points. 

*For example, imagine you have exactly two points and you want to conduct a linear regression. You need a minimum of those two data points to estimate an intercept and a slope. So if you had just one data point, you would not be able to use this type of model, and if you have more than two data points, what's left over gives you more freedom to better model your data with more information. This the general idea of degrees of freedom.*

Let's again check this in r. 

```{r, looking at the variance}
# getting the hat matrix 
H <- X.matrix%*%solve(t(X.matrix)%*%X.matrix)%*%t(X.matrix)

# doing it with a sum 
sum((Yi - H%*%as.matrix(Yi))^2)/(N-2)

# doing it the matrix way 
e.dot <- Yi - H%*%as.matrix(Yi)
(t(e.dot)%*%e.dot)/(N-2) # just a vector of sigmas

# looking at the variance from the linear model fit 

```

We see both

```{r}
# sigma.hat <- 

# 
# sigma.hat <- matrix(c(sigma.hat,0,0,sigma.hat), nrow = 2) # sigma*Identity matrix
# C <- solve(t(X.matrix)%*%X.matrix)
# 
# # note that the residual standard error from lm should match these diagonals  
# sqrt(sigma.hat)
# 
# # these should also match. ADD A DESCRIPTION AS TO WHY 
# varcovar.matrix <- sigma.hat%*%C
# varcovar.matrix
# vcov(lm.fit) # compare with above 
# 
# # we should get the same estimates of the SE from the lm 
# sqrt(varcovar.matrix[1,1])
# sqrt(varcovar.matrix[2,2])
```

### Back to hypothesis testing

So now we have the solution to the least squares equations. We have a vector of $\beta$'s, as well as an estimate for $\sigma$ which in turn gives us the standard error for the $\beta$'s.

But remember our goal: our goal is to see if there is any linear relationship between our independent variable $X_i$ and our dependent variable $Y_i$. We can formulate this into a hypothesis test. The general steps are... 1. Create the null and alternative hypotheses. 2. Make an assumption about the processes that give rise to your data and choose a test statistic that will summarize the data that addresses this assumption. Note that this test statistic will come from a specific distribution. You can't mix and match test statistics with distributions. Test statistics, based on how they are formuulated, will have certain distributions that they come from. 3. Decide a threshold or significance level, $\alpha$ such that if the probability of getting the test statistic that we have given our null distribution is as extreme or more, we reject the null hypothesis. This also known as the **p-value**.

### Quick general facts about some test-statistic distributions

Note that there are test statistics that come from different distributions, but in general, we have the t-distribution, F-distribution, and $\chi^2$ distribution.

### Calculating our test statistic!

Because in this case,

```{r, calculating our test statistic and p-value}

# beta0_hat <- betas[1,1]
# beta0_SE <- sqrt(varcovar.matrix[1,1])
# beta1_hat <- betas[2,1]
# beta1_SE <- sqrt(varcovar.matrix[2,2])
# 
# # beta2_hat <- lm.fit$coefficients[[3]]
# # beta2_SE <- summary(lm.fit)$coefficients[, 2][[3]]
# 
# # we should get the same estimates of the t-value and p-value from the lm
# 2*(1-pt(abs(beta0_hat/beta0_SE), N-2)) # beta0_hat/beta0_SE is the t-value 
# 2*(1-pt(abs(beta1_hat/beta1_SE), N-2)) # or 2*pt(beta1_hat/beta1_SE, N-2), so we do abs() to account

```

Confidence intervals, p-values, linear least squares.

How do we estimate

How does the Central Limit Theorem tie into this?

How are test statistics and confidence intervals related?

### Looking at additional information

$R^2$

```{r, looking at r^2}
# SST <- sum((Y1 - mean(Y1))^2) # sum of residuals from the mean, df = n-1
# SSE <- sum((Y1 - H%*%as.matrix(Y1))^2) # sum of residuals from best fit line, df = n-p
# SSR <- SST - SSE # the difference, df = p-1. based on pythagorean theorem  
# 
# # this should match the R-squared
# SSR/SST # also can be written as 1 - SSE / SST
# 
# # NEED TO FIGURE OUT ADJUSTED R^2
# # Adjusted R-squared:  0.3764 
# # we can adjust using the degrees of freedom, SSR/(p-1) / SST/(n-1)
# SSR/(SST/(N-1))
# (SSR/SST)*(N-1)
```

```{r, the f-statistic}
# this should match the f-stat p-value from the lm 
# 2*(1-pf(abs(SSR/(SSE/(N-2))), 1, N-2)) # like the t-dist, SSR/(SSE/(N-2)) is the f-statistic
```

### For future

-   Bernoulli/Binomial distributions, Poisson distributions, interactions

### Additional resources

-   On matrix properties: https://www.ucl.ac.uk/\~ucahhwi/MATH6502/handout9.pdf
-   More on the hat matrix: https://pj.freefaculty.org/guides/stat/Regression/RegressionDiagnostics/OlsHatMatrix.pdf
-   More on simple linear regression with matrices: https://www.stat.cmu.edu/\~cshalizi/mreg/15/lectures/13/lecture-13.pdf
-   On a geometric interpretation: https://bookdown.org/ts_robinson1994/10EconometricTheorems/linear_projection.html

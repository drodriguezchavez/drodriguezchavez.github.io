---
title: "Maximum Likelihood"
date: 04/30/2024
Keywords: "MLE, Hessian, regression"
---

```{r, echo = FALSE, include = FALSE}
library(tidyverse) 
library(ggplot2)
library(latex2exp)
```

To start, let's do the first thing many do when trying to learn about a new topic: go to the [Wikipedia page](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) for it. Let's start with that first paragraph:

*In statistics,* *maximum likelihood estimation (MLE) is a method of estimating the **parameters** of an **assumed probability distribution**, given some **observed data**. This is achieved by maximizing a **likelihood function** so that, under the assumed statistical model, the observed data is most probable. The point in the **parameter space** that maximizes the likelihood function is called the [maximum likelihood estimate]{.underline}. The logic of maximum likelihood is both intuitive and flexible, and as such the method has become a dominant means of statistical inference. (Taken directly from Wikipedia.)*

Hopefully if it's not intuitive as of yet, it will be once we work through this. Let's break down this first paragraph into various components: **parameters**, **assumed probability distribution**, **observed data**, **likelihood function**, and **parameter space**.

Let's first take about the **assumed probability distribution**. This is the type of distribution you *assume* your data is coming from. For example, for count data, we often use the Poisson distribution, denoted $$ Pois(\lambda) = \frac{\lambda^k e^{-\lambda}}{k!} $$ where $k$ is the number of events that occur in an interval given by rate $\lambda$.

In the above example, $\lambda$ is what we call a **parameter**. This is a value that describes, in a Frequentist sense, the "true value" of your population.

```{r, echo = FALSE}
vals <- seq(0,50,1)

ggplot() +
  ggtitle('Examples of Poisson distributions') +
  geom_line(aes(x = vals, y = dpois(vals, lambda = 2), color = "2")) + 
  geom_line(aes(x = vals, y = dpois(vals, lambda = 4), color = '4')) +
  geom_line(aes(x = vals, y = dpois(vals, lambda = 8), color = '8')) +
  geom_line(aes(x = vals, y = dpois(vals, lambda = 16), color = '16')) +
  geom_line(aes(x = vals, y = dpois(vals, lambda = 32), color = '32')) +
  labs(x = "Counts", y = expression(Pr(K == k))) + 
  scale_color_manual(name = expression(lambda),
                     breaks = c("2", "4", "8", "16", "32"),
                     values = c("2" = "red", "4" = "orange", 
                                "8" = 'green', "16" = "blue", "32" = "purple")) 

```

Often, we are trying to figure out what the best parameter value fit for our data is, to try and get at the "true value" of where our data comes from. Say you want to know the average amount of chicks are in a clutch laid by the [Black-crowned Night Heron](https://www.allaboutbirds.org/guide/Black-crowned_Night_Heron/id#) in your neighborhood (which are found aplenty in Lake Merritt, a home to many a [strange creature](https://baynature.org/biodiversity/enigmatica/)). The most obvious way to do this, would be to measure all of the chicks in each clutch! Then you would get the "true value" of the population mean. But, you realistically can't measure all of the baby birds in the area. That's a lot of time and patience, as well as luck that you simply don't have. But! You do have a couple of Night Heron clutches that you found lying around, so you counted the amount of chicks in each of those. So this is your **observed data**. You then use this data to figure out what your best guess for a parameter would be.

```{r, echo = FALSE}
set.seed(1)
clutch <- c('Clutch 1', 'Clutch 2', 'Clutch 3', 'Clutch 4', 'Clutch 5',
            'Clutch 6', 'Clutch 7', 'Clutch 8', 'Clutch 9', 'Clutch 10')
size <- rpois(10, lambda = 4)
dat <- as.data.frame(size, clutch);dat
```

Now, let's do some *assuming* here (the Frequentist view). Because we can only have a non-negative number of baby chicks and they are integers (you can't have 0.1 of a baby chick) let's *assume* that the clutch sizes come from the Poisson distribution, the one we mentioned earlier. Let us also *assume* that each clutch is **independent and identically distributed**, meaning that each clutch size does not depend on another and we *assume* that all of them come from the same distribution.

```{r, echo=FALSE}
ggplot() + 
  geom_point(aes(x = 1:10, y = dat$size), size = 4) + 
  ylim(c(0,8)) +
  xlab("Clutch #") + 
  ylab("Size of each clutch") + 
  scale_x_continuous(breaks = 1:10) + 
  ggtitle("Visualizing the size of each clutch")

```

Great, but so what? We have decided that the **observed data** come from a Poisson **distribution**, but we still need to find the actual **parameter** $\lambda$ for this species. Well, luckily, because we've assumed our data is **independent and identically distributed**, we can treat each point (i.e. each clutch size) as independent events. Now, statistics is cool and has the rule that if $A$ and $B$ are the probabilities of two independent events, then the probability of both happening is $A*B$ (think of how the probability of getting two heads after two fair coin flips is $0.5*0.5$ because the probability of getting heads is $0.5$ and each fair coin flip is independent of any other fair coin flip).

So, keeping that rule in mind, we can get the probability that these 10 clutch sizes would happen by multiplying the probability of each one. Now, you might be thinking, we don't know the probability of each one. But we do! Kind of. We know that they come from the Poisson **distribution**. Which is $$ Pois(\lambda) = \frac{\lambda^k e^{-\lambda}}{k!} $$

Now, I know what you're thinking: we still don't know $\lambda$! And this is true. But, when we multiple all of these together with our various $k$ (i.e. our **observed data**, which is our clutch sizes), we will have a **probability** **function** from our data that relies on $\lambda$. As we test different $\lambda$, we will get different values.

$$f(\lambda) = \Pi_1^n \frac{\lambda^{k_i} e^{-\lambda}}{k_i!}$$

*Note that* $n$ *is our number of data points and* $k_i$ *represents a single data point value.*

As we know, the higher the number from a probability function, the more probable it is for that scenario or set of conditions to happen. In this case, we want to find the value of **parameter** $\lambda$ from our **function** we have just created, that is the most **likely**. Can you guess the official name of our function now? It's the **likelihood function**!

```{r, echo = FALSE}

```

Now there are a lot of assumptions in statistics and this took me a while to get used to.

Do you have to differentiate every likelihood function you come up with? No!

Note that we usually use a log scale for the

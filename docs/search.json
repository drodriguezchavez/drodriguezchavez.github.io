[
  {
    "objectID": "music-theater.html",
    "href": "music-theater.html",
    "title": "Musical Theater",
    "section": "",
    "text": "Recent Shows I’ve Seen\nJust came back from the Edinburgh Fringe!\n\n\nRecent Shows I’ve Been In\nUPCOMING FALL 2025: Cantos de Mi Tierra at Martuni’s Piano Bar, A Musical Theater Revue at Berkeley Playhouse, Unplugged Cabaret at The Live Oak Theater.\n\nAugust 2025: The Family Copoli at TheSpaceUK on Niddry St. at the Edinburgh Fringe Festival\nSpring 2025: Cantos de Mi Tierra at Martuni’s Piano Bar and Into the Woods Act One at The Live Oak Theater\nNovember 2024: A Musical Theater Revue at Berkeley Playhouse\nJune 2024: Baby Rock at the Hollywood Fringe Festival\nMay 2024: Just My Type at Chanticleer’s Theatre\nOct-Nov 2023: Tintypes at Contra Costa Civic Theatre\nMay 2023: Family Copoli at Cornell University\n\n\n\nTheaters I Recommend\nThere are a lot of great theaters in the Bay Area. Below is a small subset that I frequent. However, for a more full list checkout Theatre Bay Area or my own personal list (where you can see which shows I’m planning on attending).\n\nThe Orpheum, Golden Gate Theater, Curran Theatre. These are all easily accessible by BART at show touring Broadway shows. Check out NightOutNightOff for cheaper tickets to these shows.\nThe Gateway Theatre. Easily accessible by BART and have cheap tickets on TodayTix.\nBerkeley Playhouse. Easily accessible by AC Transit and very close to campus.\nBerkeley Rep. Easily accessible by BART. Does incredible shows but can get a little pricey. Try the Tuesday 12pm ticket releases for $20 tickets.\nShotgun Players. Easily accessible by BART and AC Transit. Tend to have very community engaging shows.\nSan Francisco Playhouse. Easily accessible by BART. Really great quality and somehow manages to do during the week shows (except Mondays). Highly recommend their $20 rush tickets which open 30min before show."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Daniela Rodriguez-Chavez",
    "section": "",
    "text": "Growing up, I always enjoyed math. It was fun to solve these puzzles and figure out what I can create with a set of given rules. In addition to this passion, I also enjoyed playing the piano, a requirement for many children of immigrant parents. By the time I entered high school, I had decided I was going to major in math and music… with no plan for what I would do after college. However, two crucial things happened my senior year: I took AP Environmental Science (APES) and I music directed a show for the first time.\nAfter taking APES, I knew that I had to combine math and environmental science somehow, but I wasn’t sure how. I took on a motto: “help someone and something, somewhere.” This led me on a 4-year journey into carbon cycle dynamics, oceanography, and population dynamics, until I ended up in the world of infectious disease modeling. Meanwhile after music directing, I realized that this was how to keep my creative skills alive while being in community with others. I ditched the music major and doubled in mathematics and environmental science at Cornell University.\nGraduate school was not a clear step to me until my third year of college. As a child of Mexican immigrants, I rarely saw myself represented in the mentors and professors I had. Though I am eternally grateful to everyone that helped guide me down this path, I am really passionate about the communities I’m a part of and I hope to be the mentor that my younger self wanted.\nWant to create your own free website? First create a quarto website on RStudio and make it a git repo. You then want to connect it to Github (learn more about Git here) and make sure you can publish it. Woohoo now you have your own free website!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Daniela Rodriguez-Chavez",
    "section": "",
    "text": "Hello! My name is Daniela and I am an NSF Graduate Research & Chancellor’s Fellow at UC Berkeley in both the de Valpine and Boettiger labs. I’m really interested in infectious disease modeling and I plan to use this time to develop skills in statistical, computational, and mathematical modeling.\nOn this site you’ll get to know a little bit more about me, my research, and some of my community work. Because I am currently learning a lot of material on my own, I also intend to provide some write-ups in case that they are useful to anybody. Outside of academia, I enjoy spending my time in the world of musical theater and I’m always happy to catch a show with someone new!"
  },
  {
    "objectID": "stat-blog.html",
    "href": "stat-blog.html",
    "title": "Learn Statistics With Me",
    "section": "",
    "text": "Title\n\n\nKeywords\n\n\nDate\n\n\n\n\n\n\nSimple Linear Regression and Hypothesis Testing\n\n\n \n\n\nSep 24, 2025\n\n\n\n\nOnline Resources\n\n\nindependent learning, mathematics, statistics\n\n\nSep 13, 2025\n\n\n\n\nMaximum Likelihood\n\n\nMLE, distribution, regression\n\n\nAug 5, 2024\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "A classmate of mine in undergrad once told me that there are models that fit functions to data and then there are models that describe a theoretical framework but may not match data perfectly. The former by definition needs data while the latter can use data to corroborate theory. I am interested in gaining skills to do both of these!\n\nCurrent Work\nCurrently, I am exploring the spatio-temporal relationships between dengue and zika in Brazil from 2016-present. I am interested in learning how statistical models can better understand these interactions for forecasting and use dynamical systems modeling to better understand multi-pathogen systems. This work is being done in collaboration with Ayesha Mahmud.\n\n\nPublications\n\nde Marez, C., J. Callies, B. Haines, D. Rodriguez-Chavez, and J. Wang, 2023: Observational Constraints on the Submesoscale Sea Surface Height Variance of Balanced Motion. J. Phys. Oceanogr., 53, 1221–1235, https://doi.org/10.1175/JPO-D-22-0188.1.\nLee, S., D. Rodriguez-Chavez, and J. Rzeszotarski, 2023: Exploring the Effects of Personal Impact Communicated Through Eco-Feedback Technology for Reducing Food Waste. In: Marcus, A., Rosenzweig, E., Soares, M.M. (eds) Design, User Experience, and Usability. HCII 2023. Lecture Notes in Computer Science, vol 14030. Springer, Cham. https://doi.org/10.1007/978-3-031-35699-5_39\n\n\n\nPresentations\n\nDecember 2025: TBD, Epidemics\nJune 2025: “Testing theories of cross-immunity and cross-reactivity on mosquito-borne diseases using a dynamical systems approach” at EEID\n\n\n\nAwards\n\n2023 National Science Foundation Graduate Research Fellow\n2023 UC Berkeley Chancellor’s Fellow\n2022 Poster Winner at Los Alamos National Laboratory Student Symposium\n2022 Best Poster at Int. Symposium on Physiological Processes in Roots of Woody Plants\n2021-2022 Recipient for the Ann S. and Robert R. Morley Student Research Fund Grant\n2021 Best Undergraduate Presenter at AGEP Student Success Conference\n\n\n\nPrevious Work\n\nUndergraduate researcher in the Department of Natural Resources & The Environment at Cornell University. Advised by Marc Goebel.\nComputational Sciences Intern at the Los Alamos National Laboratory Information Systems and Modeling (A-1) Group. Mentored by Morgan Gorris and Andrew Bartlow.\nWAVE Fellow at the California Institute of Technology. Mentored by Joern Callies."
  },
  {
    "objectID": "community.html",
    "href": "community.html",
    "title": "In Community",
    "section": "",
    "text": "Though research can be isolating, I’m a firm believer in creating community, whether it’s inside or outside of the classroom. “It takes a village” is definitely true and I hope to make both my local and broader community a better place.\n\nLocal Berkeley Community\n\n2023-present: ESPM Wiki/Slack/WhatsApp Coordinator\n2023-present: ESPM Alumni Coordinator\n2024-2025: ESPM Graduate Student Association Co-President\n2023-2025: PubScience Organizer\n\n\n\nTeaching\n\nFall 2022 TA for Applied Population Dynamics at Cornell University\nFall 2021 TA for Field Biology at Cornell University\n\n\n\nOrganizations\n\nSACNAS\nWomen of Color in EEB"
  },
  {
    "objectID": "stat-blog/maximum-likelihood.html",
    "href": "stat-blog/maximum-likelihood.html",
    "title": "Maximum Likelihood",
    "section": "",
    "text": "To start, let’s do the first thing many do when trying to learn about a new topic: go to the Wikipedia page for it. Let’s start with that first paragraph:\nIn statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of an assumed probability distribution, given some observed data. This is achieved by maximizing a likelihood function so that, under the assumed statistical model, the observed data is most probable. The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate. The logic of maximum likelihood is both intuitive and flexible, and as such the method has become a dominant means of statistical inference. (Taken directly from Wikipedia.)\nHopefully if it’s not intuitive as of yet, it will be once we work through this. Let’s break down this first paragraph into various components: parameters, assumed probability distribution, observed data, likelihood function, and parameter space.\nLet’s first take about the assumed probability distribution. This is the type of distribution you assume your data is coming from. For example, for count data, we often use the Poisson distribution, denoted \\[ Pois(\\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!} \\] where \\(k\\) is the number of events that occur in an interval given by rate \\(\\lambda\\).\nIn the above example, \\(\\lambda\\) is what we call a parameter. This is a value that describes, in a Frequentist sense, the “true value” of your population.\nOften, we are trying to figure out what the best parameter value fit for our data is, to try and get at the “true value” of where our data comes from. Say you want to know the average amount of chicks in a clutch laid by the Black-crowned Night Heron in your neighborhood (which are found aplenty in Lake Merritt, a home to many a strange creature). The most obvious way to do this, would be to measure all of the chicks in each clutch! Then you would get the “true value” of the population mean. But, you realistically can’t measure all of the baby birds in the area. That’s a lot of time and patience, as well as luck that you simply don’t have. But! You do have a couple of Night Heron clutches that you found lying around, so you counted the amount of chicks in each of those. So this is your observed data. You then use this data to figure out what your best guess for a parameter would be.\nsize\nClutch 1     3\nClutch 2     6\nClutch 3     5\nClutch 4     3\nClutch 5     9\nClutch 6     9\nClutch 7     3\nClutch 8     7\nClutch 9     5\nClutch 10    5\nNow, let’s do some assuming here (the Frequentist view). Because we can only have a non-negative number of baby chicks and they are integers (you can’t have 0.1 of a baby chick) let’s assume that the clutch sizes come from the Poisson distribution, the one we mentioned earlier. Let us also assume that each clutch is independent and identically distributed, meaning that each clutch size does not depend on another and we assume that all come from the same distribution.\nGreat, but so what? We have decided that the observed data come from a Poisson distribution, but we still need to find the actual parameter \\(\\lambda\\) for this species. Well, luckily, because we’ve assumed our data is independent and identically distributed, we can treat each point (i.e. each clutch size) as independent events. Now, statistics is cool and has the rule that if \\(A\\) and \\(B\\) are the probabilities of two independent events, then the probability of both happening is \\(A*B\\) (think of how the probability of getting two heads after two fair coin flips is \\(0.5*0.5\\) because the probability of getting heads is \\(0.5\\) and each fair coin flip is independent of any other fair coin flip).\nSo, keeping that rule in mind, we can get the probability that these 10 clutch sizes would happen by multiplying the probability of each one. Now, you might be thinking, we don’t know the probability of each one. But we do! Kind of. We know that they come from the Poisson distribution. Which is \\[ Pois(\\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!} \\]\nNow, I know what you’re thinking: we still don’t know \\(\\lambda\\)! And this is true. But, when we multiple all of these together with our various \\(k\\) (i.e. our observed data, which is our clutch sizes), we will have a probability function from our data that relies on \\(\\lambda\\). As we test different \\(\\lambda\\), we will get different values.\n\\[f(\\lambda) = \\Pi_1^n \\frac{\\lambda^{k_i} e^{-\\lambda}}{k_i!}\\]\nNote that \\(\\Pi\\) just means to multiply values together, much like how \\(\\Sigma\\) adds items together. \\(n\\) is our number of data points and \\(k_i\\) represents a single data point value.\nAs we know, the higher the number from a probability function, the more probable it is for that scenario or set of conditions to happen. In this case, we want to find the value of parameter \\(\\lambda\\) from our function we have just created, that is the most likely. Can you guess the official name of our function now? It’s the likelihood function! Note that for computational reasons, you will often see the log scale version of this.\n\\[L(\\lambda) = \\sum_1^n (log(\\lambda^{k_i}) + log(e^{-\\lambda}) - log(k_i!)) \\] \\[ = \\sum_1^n (k_i *log (\\lambda) -\\lambda - log(k_i!) )\\] Plugging in a few values, we see a general curve appear below!\nThis graph above is a visualization of what we call the parameter space. We are seeing where the maximum is along a set of guess parameter values. Looking at our log-likelihood function above (or surface if it’s two parameter values, such as in the normal distribution), we see that a \\(\\lambda\\) value of around 5.5 might be the maximum.\nUnsatisfied with this? Is there a way to get the exact maximum without just looking at the graph and guessing? If you ever took calculus, you’ll remember that to find the maximum of a function, you can take the derivative of the function, set it to 0, and find the parameter value that satisfies that equation.\n\\[ L'(\\lambda) = \\sum_1^n (\\frac{k_i}{\\lambda} - 1) = 0 \\]\n\\[ \\sum_i^n k_i/n = \\hat{\\lambda} \\]\nWhich in our case does equal 5.5! Note that the hat on \\(\\lambda\\) is used to denote it as an estimator.\nWe can now happily complete our inquiry and be confident that our birds are likely delivering on average a clutch size of 5.5/year, Poisson distributed. But this isn’t an integer, you may be inclined to say. Well, \\(\\lambda\\) is an average. If you saw two dogs one day and then three dogs the next, you would be seeing an average of 2.5 dogs a day even though it’s physically impossible (or just really sad) to see half a dog!\nHowever two questions immediately arise once we start to deal with more complicated data and distributions.\nBoth of these questions can be answered the same: optimization! In reality, no pencil-and-paper differentiation is used. Your regular linear regression in R uses these techniques to find the intercept and slope of the regressions. Then, there are other packages such as nimble that use a method called MCMC under a Bayesian framework that will return to you parameter estimates using the same principles as defined here, but it can be much harder to imagine, for example, a 4d space, so it’s nice that they can do it for us!"
  },
  {
    "objectID": "Publications.html",
    "href": "Publications.html",
    "title": "Publications",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "gschool.html",
    "href": "gschool.html",
    "title": "Grad School Guide",
    "section": "",
    "text": "What is graduate school?\nGraduate school can mean many things to many folks. In general, it’s a program from which you attend after attending an undergraduate institution and you receive a degree. There are specific graduate programs such as for medicine, dentistry, and law. I will not be touching on these since those have specific sets of requirements and application processes that will not be covered on this page.\nThere are also master’s programs and doctoral programs. Within master’s degrees, there are generally two branches of programming: professional or research-based. In a professional master’s program, there is an emphasis on coursework and skill-building, potentially with an internship component. Whereas in a research-based master’s, there is an emphasis on coursework but also the presentation of a master’s thesis, which can get published. Again, these are generalizations, and always check with whatever program you are interested in to learn more about the specifics.\nIn doctoral programs, the emphasis is on contributing original research to the field, with two main components: a qualifying exam and a dissertation (often with a defense). In the United States, these degrees take around 5 years to complete (depending on the field some can take as little as 4 or as much as 8+ years).\nAnother main point to note is that while most graduate programs require payment to attend, doctoral degrees are usually paid. You should never pay to attend a doctoral program. Note that the pay will be different whether you choose to attend a public or private university, but there should be pay regardless.\nThe rest of this article is generally for applying to PhD programs, since that is what I am most familiar with.\n\n\nHow do I know if I want to go?\nGraduate school, especially a doctoral program, is not something to be taken lightly. Here, I tried to distill the two main areas that I thought about before applying to graduate school.\n\ncareer goals\nIt’s helpful to keep in mind what type of career you generally want to have. If getting a PhD is on the way to getting that career, that’s an indication that graduate school might be a good fit. But if getting a PhD is not necessary for a specific career, that does not mean you shouldn’t go to graduate school, it just means you have to think harder about why a PhD is necessary.\nIn my mind, I view a PhD as a time in your life where you are given the opportunity to build specific skills and learn about a specific topic you are interested in. You want to use that time wisely. Please, please, do not think that getting a PhD “pigeon-holes” you into doing anything. Obviously if you get a PhD in English Literature, it might be harder to become a surgeon, but you could say that about many things. It’s not so much that a PhD blocks you from certain career paths, but rather moves you to be more likely to do certain career paths, such as a research scientist (whether public or private sector), professor, or even science-communication related roles such as policy-advisor, journalism, advocacy, etc. (Not to say you can’t do many of these jobs without a PhD, but it certainly makes things easier.)\nHowever, the main thing about getting a PhD, is that you must conduct research, in whatever form that may take depending on the field.\n\n\nresearch\nThe core backbone of a PhD is the research you produce. So, I would recommend doing research before applying to graduate school to see if you like it. It is also helpful to reference projects you’ve done and how they’ve shaped your research journey when applying to graduate school. Here are some ways that I got into research:\n\nIs there a topic you’re interested in from a class? Reach out to the professor to see what other work has been done in it.\nDid your professor mention a project they were working on in class? Reach out to see if you can help.\nCheck out Research Experiences for Undergrads (REUs), these were a great way to spend a summer feeling what research can be like!\nAre there clubs on campus that do research? Chat with some folks or apply and see if they pique your interest.\n\nResearch is hard! It’s squishy. It doesn’t have to be a 9-to-5 job. It can take a lot of stamina. In any job there are going to be things you don’t enjoy, but make sure to figure out what you do and don’t like. It’s also okay to try out different research areas, especially if you’re coming from a quantitative field! I personally knew I liked research because I enjoyed the flexibility of working (generally) on my own schedule and I enjoyed applying my quantitative skills to a new problem that I got to ask. But it took me a while (about three years) to really find an area of research I really enjoyed.\n\n\n\nHow do I apply to graduate school?\nSo you’ve now decided to apply to graduate school, congrats! You’ve clearly thought about it which is going to be helpful for the application process. Now, every graduate program has a website where they detail the requirements. But, there are also unspoken rules that (I think) helped me significantly increase my chances of being accepted.\n\nthe usual requirements\nAt this point in time, the Graduate Record Examinations (GRE) is mostly optional when applying. I did not take it, but I know some people did and you should only include it if it can help you.\nIn addition to the GRE, there’s also the Statement of Purpose (SOP). Think of your SOP as a way for you to sell your story. You’ve thought about why you want to do a PhD quite a bit at this point! Here’s where you can craft the narrative to make a probably winding journey seem very natural. You’ll want to incorporate not only how you got interested in the research path you intend to take at the program, but also what you learned and how you grew from different research experiences. Make sure to mention why you need this specific program to help achieve your goals. It is very helpful to mention a few specific professors that you would like to work with and why.\nFinally, there are recommendation letters. You typically ask 3-4 people for them. These folks don’t all have to be professors, but they do need to be able to talk about your achievements and the work you’ve done. Examples can be professors you worked with at an REU, an academic advisor you’ve developed a relationship with, or a professor whose class you took or TA’d. You want to make sure that you ask them well in advance of the deadlines (around two-three months) and keep track of them! Some folks might take a while to respond (more than four weeks with one or two follow-ups), which should be viewed as a soft no. But for those that do agree, you should assume you are responsible for making sure they get it in on time. Send one or two reminders as the deadline approaches and check in the application portal to make sure the email is correct.\nNote that some applications might now have you do a diversity statement as well, where you can draw on your own life and talk about outreach you might do and contribute to the diversity in the program and field. Also check out my “personal graduate guide” to see my SOPs for which I was accepted.\n\n\nthe unspoken rules\nIn your SOP, you’ve mentioned some professors that you would like to work with. But, this should not be the first time they’ve seen your name! At least three-four months before you apply to an institution, you should reach out to a few faculty about chatting with them about their research. This can help in many ways:\n\nServes as a vibe-check. This would be your advisor for at least five years. Do they seem nice? Did they seem interested in chatting with you?\nThey get to know you. They see that you are interested and are wanting and willing to chat with them.\n\nAlso don’t be afraid to talk to people about your application process! Let your academic or research advisors know, let your professors know, they might be able to connect you with someone they know that they think would be a good fit or offer additional advice.\nNote that some programs will not recommend you chatting with professors, but it cannot hurt to reach out, especially in lab-admit programs where there is no rotation aspect (i.e. you don’t try out many different labs).\n\n\napplication fees\nThere are also application fees, but there might be ways to get waivers for them.\n\nTry to go to the SACNAS conference the semester you are applying to grad school. This would mean applying the summer before either as an attendant or as a participant where you could present research you are working on! Hopefully the lab you are working with could help sponsor, but if not check to see if your undergrad institution offers conference funding and SACNAS often offers a travel scholarship. Once you are there, you should have a list of schools you are applying to. There will be a bunch of booths of these universities at SACNAS. Find them and ask if they have any fee waivers. This is also a great time to ask them questions! See some example questions in the following section and also in my “personal grad school guide.”\nEmail the graduate admissions officer or look around on the website to see if there are any opportunities for fee waivers. Often these are for folks who attended an REU, belong to an affinity group, and/or demonstrate financial need.\n\n\n\nbe organized\nThere’s a lot to gather. There are professors to email, applications to write, recommendation letters to request, and deadlines to meet. Figure out what way would help you best in getting organized (spreadsheet, Google Calendar, etc.). I have an example spreadsheet in “Additional resources.”\n\n\n\nQuestions to ask during the process\nOne of the biggest things about applying to graduate school for me was that there was a lot I didn’t know. And there was a lot that I didn’t know I didn’t know. Below are some questions that are helpful to ask yourself throughout the process, as well as some additional ones to ask potential advisors and colleagues. Note that in the “Additional resources” section I have a document containing a list of questions to keep in mind when interviewing at places.\n\nto yourself\n\nHow much does location matter? weather? urban vs. rural? cost of living?\nHow much do I care about the amount I make after graduating college?\nWhat kind of support do I need to succeed?\nWhat do I want to get out of a PhD?\nWhat do I want to able to have done in 4-7 years by being in this program?\n\n\n\nto your potential advisors\n\nWhat are your expectations at individual meetings?\nHow often are you able to meet with students?\nWhat are your thoughts on how the first two years should go in a PhD program?\nHow do you support your students in their career and throughout their PhD?\nWhat is funding like in the program?\n\n\n\nto your potential colleagues / labmates\n\nWhat was your thought process in deciding to go here? Why this lab?\nHow were the qualifying exams for you?\nWhat do you have to deal with in this program? What makes you frustrated?\nWhat is it like to be a woman or woman of color in this program? replace with another affinity group if necessary\nHow supportive is your advisor? career-wise? personally?\nHow would you describe the lab community? the program community?\nWho helps you with the requirements and navigate the program?\nWhat is the work-life balance like in your lab / in the program?\nWhat do you do for fun? How do you destress?\n\n\n\n\nFunding\nThis last section is on funding. As I mentioned earlier, PhD’s should always be funded in the United States. However, the stipulations of that funding depend on the department. Some departments give funding with no strings attached; others will ensure you get a specific amount of funding but you may have to teach or perform research on a grant that will not apply to your dissertation. It’s important to know what type of funding you could receive. That being said, look for fellowships! The Graduate Research Fellowship Program is a common one, but Google others, ask around for others. Note that I have my and other GRFP applications in “Additional resources.”\n\n\nAdditional resources\n\nMy personal graduate school guide containing my SOPs, GRFPs, an organizational spreadsheet, and list of questions, among other things.\nDemystifying the Graduate School Application Process, a paper published by folks from UC Berkeley’s Environmental Science, Policy, & Management Department demystifying some of the unspoken rules of academic.\nWorkshop packet on applying to graduate school, put together by UC Berkeley’s Environmental Science, Policy, & Management Graduate Diversity Council and the Plant and Molecular Biology department."
  },
  {
    "objectID": "stat-blog/online-resources.html",
    "href": "stat-blog/online-resources.html",
    "title": "Online Resources",
    "section": "",
    "text": "It took me a very long time to become comfortable with using anything other than my textbook and notes when learning concepts in school. However, I’ve learned that I best understand something when I can visualize it and clearly write out all the steps. Because of this, YouTube has been an amazing resource, as well as other online sources where I can see how other people interpret and explain these concepts.\n\n3blue1brown - Originally a YouTube channel created by Grant Sanderson, 3blue1brown is a great resource for learning any type of maths with intuitive visualizations. In addition, he will sometimes do one-off videos on cool concepts in mathematics and has know started having guest videos!\nPaul’s Online Math Notes - Paul’s Online Math Notes helped me a lot in high school to learn calculus as well as differential equations. With clear explanations and also so practice problems with solutions, it’s a resource that I sometimes go back to if I want to brush up on foundations of mathematics.\nVery Normal – This is an amazing YouTube channel (that also has a substack of the same name) by recent Ph.D graduate Christian. Really clear and well-animated videos of many different statistical concepts, this channel has helped me a lot!\nStatistics for Ecologists - Written by John Fieberg at the University of Minnesota, this is an amazing textbook with a lot of practical exercises. I went through the whole book and it helped me gain a good foundation of statistical modeling.\nPrimer - Though they rarely make videos, Primer is a YouTube channel with great animations that explains some basic statistical concepts.\nNumberphile - This is a great YouTube channel to see professors and mathematicians explain problems in real time. Rarely any animations, it’s a lot of pen to paper explaining and working through a problem.\nFlammable Maths - Though 2025 is his last year on YouTube, this is a fun channel solving math problems from algebra to calculus."
  },
  {
    "objectID": "stat-blog/maximum-likelihood.html#other-notes",
    "href": "stat-blog/maximum-likelihood.html#other-notes",
    "title": "Maximum Likelihood",
    "section": "other notes",
    "text": "other notes\nWhat is Frequentist vs. Bayesian? This is a great rabbit hole to dive into! My general understanding is that a Frequentist viewpoint comes from the idea that there is a “true” parameter value that is fixed and we are trying to uncover from our observed data. The Bayesian framework comes from the idea that our parameter values themselves are random variables that come from a distribution, which we can specify (this is called your prior distribution) to hopefully gain a better understanding of where the data comes from. Recommended reading: Statistical Modeling: The Two Cultures by Breiman (2001) and Bayesians, Frequentists, and Scientists by Efron (2005).\nBut the real world isn’t independent and identically distributed! Now there are a lot of assumptions in statistics and this took me a while to get used to. It’s a very different language to learn. What helped me “buy into the system,” so to speak, is to really take in the quote “all models are wrong, but some are useful.” Statistics does not describe the real world down to every blade of grass, but it can help us to uncover trends and give us the best tool we have to estimate parameters that have very real implications in fields such as healthcare and the environment (i.e. setting thresholds of toxicity, etc.). Recommended reading: Data Detective by Tim Harford.\nI still have thoughts and questions! Feel free to let me know how I can improve this post with other resources by clicking the mailbox icon at the bottom :)"
  },
  {
    "objectID": "stat-blog/mcmc.html",
    "href": "stat-blog/mcmc.html",
    "title": "Monte Carlo Markov Chains (MCMC)",
    "section": "",
    "text": "MCMC"
  },
  {
    "objectID": "stat-blog/maximum-likelihood-ii.html",
    "href": "stat-blog/maximum-likelihood-ii.html",
    "title": "Maximum Likelihood II",
    "section": "",
    "text": "MLE Part II\nIn the first Maximum Likelihood post,\n\nConfidence Intervals\nThe Hessian\nVisualization of slope and curvature"
  },
  {
    "objectID": "stat-blog/distributions.html",
    "href": "stat-blog/distributions.html",
    "title": "Distributions",
    "section": "",
    "text": "Learning statistics is a language, and for me, learning the distributions is like learning the grammar. Once you get the reasons for why the distributions are what they are (and why the have the form they do), it simplifies the mystery of why we use certain distributions for certain problems (and if you do Bayesian inference, it can help inform you of what priors you want to use).\n\nDiscrete\nBernoulli: What is the probability I get one success with only one trial?\nBinomial: Let’s expand on the Bernoulli distribution. What is the probability I get k successes in n trials?\nGeometric: Let’s also expand on the Bernoulli distribution, except now I only care about the first success in many trials until I stop. What is the probability a success occurs on the kth trial?\nNegative Binomial: Let’s expand on the Geometric distribution now because I want to end on a certain number of successes. What is the probability r number of successes finishes accruing on the kth trial? Note that the Poisson distribution is a special case of this.\nHypergeometric: What if I have a finite number of successes to find. What is the probability that I find k successes out of r total successes, where each time I remove a success I cannot sample it again (i.e. no replacement).\nPoisson: Let’s push the Binomial distribution to its limit. What is the probability of k events happening in a given interval of time given a rate \\(\\lambda\\)? Note that for really large n and really small p, the Poisson frequency function can be used instead of the binomial.\n\n\nContinuous\nUniform: What if there is an equal chance for each probability to arise? Note that this is a special case of the Beta distribution.\nExponential: Let’s expand on the Poisson. What is the probability that another event doesn’t occur in a given time interval? Note that this is a special case of the gamma distribution.\nGamma: shape rate, used for survival times along with the Weibull distribution\nNormal: Useful for modeling random error or data that is centered around a mean.\nBeta: Useful for modeling random variables that are restricted to the interval [0, 1].\nA copula is a joint cumulative distribution function of random variables that have uniform marginal distributions. This construction points out that from the ingredients of two marginal distributions and any copula, a joint distribution with those marginals can be constructed. It is thus clear that the marginal distributions do not determine the joint distribution. Remember that if y = \\(F_x(X)\\), then y is a uniform distribution.\nconvolutions: given joint frequency function p(x,y) and x and y are indep. if we have Z + X+Y, we can rearrange our joint distribution to get the cdf and then the pdf of Z.\nUnder the assumptions just stated, the joint density of U and V is fUV(u,v)= fXY(h1(u,v),h2(u,v))|\\(J^{-1}\\)(h1(u,v),h2(u,v))| for (u,v) such that u = g1(x,y) and v = g2(x,y) for some (x,y) and 0 elsewhere.\npapers: Perry: Marti Anderson copulas, Perry 2014."
  },
  {
    "objectID": "stat-blog/hypo-test-lin-reg.html",
    "href": "stat-blog/hypo-test-lin-reg.html",
    "title": "Simple Linear Regression and Hypothesis Testing",
    "section": "",
    "text": "What is the point of linear regression?\nGiven an experiment with one independent variable and one dependent variable, linear regression is a way to estimate how much the dependent variable relates to the independent variable. Let’s write out a simple linear regression below with an intercept and one covariate (\\(X_i\\)).\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i , \\epsilon_i \\sim N(0, \\sigma^2)\n\\] The four model assumptions that we have with linear regression are…\n\nLinearity. The dependent variable and independent variable have a linear relationship.\nHomoskedasticity. This means that the variance of the errors is the same for each data point, \\(i\\).\nNormality. The errors are Normally distributed.\nIndependence. This assumes that each data vector \\(i\\) is independent from another data vector, \\(j\\). In other words, the correlation(\\(\\epsilon_i\\), \\(\\epsilon_j\\)) = 0.\n\nLet’s explore a simulated set of data to better visualize this. Simulations are nice because we can make sure our models are working as we think they should before applying it to real data. For the next bit, we will be using \\(\\beta_0 = 1\\), \\(\\beta_1 = 1\\), \\(\\sigma^2 = 20\\), and with 100 data points.\n\n# simulating our data\nN &lt;- 100\nXi &lt;- runif(min = 1, max = 10, n = N)\n\n# getting our parameters \nbeta0 &lt;- 1\nbeta1 &lt;- 1\nvar &lt;- 20\n\n# putting it together \ne &lt;- rnorm(N, mean = 0, sd = sqrt(var))\nYi &lt;- beta0 + beta1*Xi + e \n\n# note that this formulation is equivalent to the above. \n# Yi &lt;- rnorm(N, mean = beta0 + beta1*Xi, sd = sqrt(var)) \n\nplot(Xi,Yi, pch = 16) \n\n\n\n\n\n\n\n\nWhat’s our goal in linear regression? We have an independent variable \\(X_i\\), and we want to see how it relates to our dependent variable \\(Y_i\\) in this linear relationship, of an intercept and a slope. The intercept, \\(\\beta_0\\) is the average value of our outcome, \\(Y_i\\) when all of our independent variables, or predictors, are set to 0. Our slope, on the other hand, can be understood as the magnitude of the change for the dependent variable given a 1 unit increase of the predictor (independent) variable.\nSo, our goal is to minimize a line through all data \\((X_i, Y_i)\\), given our choice of \\(\\beta_0\\) and \\(\\beta_1\\). One way of thinking about how to do it is to turn it into a minimization problem. We want to minimize the distance between the line (our model-predicted values in a sense) and the data itself. If \\(Y_i\\) is our dependent variable, then we can write \\(\\hat{Y}_i = \\beta_0 + \\beta_1 * X_i\\) as our model-predicted values (our line)! Thus, our goal is to minimize \\[\nmin \\space \\Sigma_{i=1}^n (Y_i - [\\beta_0 + X_i \\beta_1])^2\n\\]\n\n\nLet’s take a quick dive into matrices for a second.\nIf we want to solve the least squares equations, it can be helpful to re-write our formula in terms of matrices. Let’s imagine \\(Y_i\\) as a vector, where each row \\(i\\) containing one observation. Below, we can see the first 10 rows of the vector \\(Y_i\\) from our simulation. This is our true data!\n\nmatrix(Yi[1:10])\n\n            [,1]\n [1,]  8.5907507\n [2,]  6.4132237\n [3,]  2.1286437\n [4,]  6.0125138\n [5,]  9.3896514\n [6,]  8.9020499\n [7,]  4.4768654\n [8,] -0.5830422\n [9,]  6.5922281\n[10,]  7.5727847\n\n\nOur goal is to know find a single value for \\(\\beta_0\\) and a single value for \\(\\beta_1\\) such that \\(\\beta_0 + \\beta_1 * X_i\\) is our best fit line to \\(Y_i\\). Note that because in this case, because we simulated the data, we technically know what \\(\\beta_0\\) and \\(\\beta_1\\) should be. But, in non-simulated data land, we won’t know neither of these values, nor the extent of the variance \\(\\sigma^2\\). We will just have our best guess at a relationship. In this case it’s a linear one: \\(\\beta_0 + \\beta_1 * X_i\\).\nLet’s re-write this line in matrix form such that for row \\(i\\), we get that \\(\\hat{Y}_i = \\beta_0 + \\beta_1 * X_i\\). (I put a hat on \\(Y\\) to showcase that it is an estimate and not a true value. It can also be known as our expected value of Y).\nRecalling our rules of matrices, we can re-write it as \\[\nE[Y] = \\hat{Y} = X\\vec{\\beta}\n\\]\nwhere \\[\n\\begin{align}\nX &=  \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n  \\end{bmatrix}\n\\\\\\\\\n\\vec{\\beta} &= \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}\n\\end{align}\n\\]\nNote that \\(X\\) in this context is called the design matrix! It is a \\(n \\times p\\) matrix, where \\(n\\) is the number of observations and \\(p\\) is the number of coefficients (\\(\\beta\\)’s) to estimate.\n\n\nSolving the least squares equation\nNow we can rewrite the least squared equation (also known as the mean squared error) as \\[\nmin (Y - X \\vec{\\beta})^2 = \\hat{\\epsilon}^2\n\\] Note that we can re-write this as \\[\n\\begin{align}\nmin (Y - X \\vec{\\beta})^2 & = min( (Y - X\\vec{\\beta})^T(Y - X\\vec{\\beta})\n\\\\\\\\ &= min (Y^TY - Y^TX \\vec{\\beta} - \\vec{\\beta}^T X^T Y + \\vec{\\beta}^T X^T X \\vec{\\beta})\n\\\\\\\\ &= min (Y^TY - 2\\vec{\\beta}^T X^T Y + \\vec{\\beta}^T X^T X \\vec{\\beta})\n\\end{align}\n\\] Because we know what \\(X\\) and \\(Y\\) are already, our goal is to find the right \\(\\beta\\)’s to minimize this equation. So we differentiate and set the equation equal to 0 to find an inflection point (either a min or a max.)\n\\[\n\\begin{align}\n0 &= \\frac{d}{d\\beta} (Y^TY - 2\\vec{\\beta}^T X^T Y + \\vec{\\beta}^T X^T X \\vec{\\beta})\n\\\\\\\\ &= - 2 X^T Y + 2 X^T X \\vec{\\beta}\n\\\\\\\\ &=  - X^T Y + X^T X \\vec{\\beta}\n\\\\\\\\ X^T Y &=  X^T X \\vec{\\beta}\n\\\\\\\\ (X^T X)^{-1} X^T Y &=  \\vec{\\beta}\n\\end{align}\n\\] Now we have our \\(\\beta\\)’s that solve our least squares equation!\n\n\nGoing back to predictions and the hat matrix\nBecause we know that \\((X^T X)^{-1} X^T Y =  \\vec{\\beta}\\), we can put it back into our equation from before of \\(E[Y] = \\hat{Y} = X\\vec{\\beta}\\) to get that \\[\nE[Y] = \\hat{Y} = X\\vec{\\beta} = X (X^T X)^{-1} X^T Y\n\\] Where this matrix \\(X (X^T X)^{-1} X^T\\) is known as the hat matrix (since it is a matrix that when applied to a vector of observed data, \\(Y\\), it will give back the expected value given the covariates \\(X\\), or the predicted values, which conventionally in notation puts a hat over the variable \\(\\hat{Y}\\)).\nCan we check this with our simulated data?\n\n# doing linear regression in r\nlm.fit &lt;- lm(Yi ~ Xi)\n# getting the coefficients from the r output\ncoef(lm.fit)\n\n(Intercept)          Xi \n  2.5305191   0.7552312 \n\n# creating our design matrix\nX.matrix &lt;- as.data.frame(Xi) |&gt;\n  # adding the column of intercepts\n  add_column(int = 1, .before = 1) |&gt;\n  as.matrix()\n\n# solving for the betas from (X^T X)^{-1} X^T\nbetas &lt;- solve(t(X.matrix)%*%X.matrix)%*%t(X.matrix)%*%as.matrix(Yi)\nbetas\n\n         [,1]\nint 2.5305191\nXi  0.7552312\n\n# graphing our line \nplot(Xi,Yi, pch = 16) \nlines(Xi, X.matrix%*%betas)\n\n\n\n\n\n\n\n\nLooks good! So to recap, we now have our beta estimates for our linear regression.\n\n\nWait, but what about our variance?\nNote that the variance of \\(Y\\) is the variance of the residuals. Because we’ve assumed that each observation is independent and identically distributed, we will have a variance-covariance matrix that is a diagonal matrix with just \\(\\sigma^2\\) along the diagonal.\n\\[\n\\begin{align}\nVar(Y) &= Var(X\\vec{\\beta}+\\vec{\\epsilon})\n\\\\\\\\ &= Var(X\\vec{\\beta}) + Var(\\vec{\\epsilon}) + 2Cov(X\\vec{\\beta}, \\vec{\\epsilon})\n\\\\\\\\ &= Var(\\vec{\\epsilon})\n\\\\\\\\ &= \\sigma^2*I\n\\end{align}\n\\]\n\n\n\n\n\n\n\nRecall that our variance \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). How do we estimate \\(\\sigma^2\\)?\nAdding on from the previous section, we have that our hat matrix is \\(H = X(X^T X)^{-1} X^T\\). So we can rewrite the linear regression and then the errors as…\n\\[\n\\begin{align}\nY_i &= \\beta_0 + \\beta_1 * X_i + \\epsilon_i\n\\\\\\\\ Y &= X \\vec{\\beta} + \\vec{\\epsilon}\n\\\\\\\\ &= HY + \\vec{\\epsilon}\n\\\\\\\\ \\vec{\\epsilon} &= Y - HY\n\\\\\\\\ &= (I- H)Y\n\\end{align}\n\\] So now we have a way of getting the residuals from our hat matrix! However, we still need to find way to estimate \\(\\sigma\\).\nAn intuitive way I like to think the variance, is that it quantifies how dispersed the data is from the mean. In other words, how far \\(Y_i\\) is from \\(\\beta_0 + \\beta_1 * X_i\\) averaged over the whole data set. Sound familiar? This is the same idea as our mean squared error equation from before. This quantity is often called sum of squared errors or residual sum of squares.\n\\[\n\\begin{align}\n\\sigma^2 \\rightarrow s^2 &= \\Sigma_{i=1}^{n} (Y_i - H*Y_i)^2 / (n-p)\n\\\\\\\\ &= \\vec{\\epsilon}^2 / (n-p)\n\\\\\\\\ &= \\vec{\\epsilon}^T\\vec{\\epsilon}/(n-p)\n\\end{align}\n\\] We use \\(s^2\\) to denote the estimated variance. Note that the bottom part of the above derivation is just how we would write in matrix form from the definition of the residuals that we showed earlier.\nWhy do we divide by \\(n-p\\) and not just \\(n\\) to average over the whole set? This is because we just have \\(n-p\\) degrees of freedom. An intuitive way to think about this is that each data point you have gives you information. And the more parameters you estimate, the more you use up these information data points.\nFor example, imagine you have exactly two points and you want to conduct a linear regression. You need a minimum of those two data points to estimate an intercept and a slope. So if you had just one data point, you would not be able to use this type of model, and if you have more than two data points, what’s left over gives you more freedom to better model your data with more information. This the general idea of degrees of freedom.\nLet’s again check this in R.\n\n# getting the hat matrix \nH &lt;- X.matrix%*%solve(t(X.matrix)%*%X.matrix)%*%t(X.matrix)\n\n# doing it with a sum \nsum((Yi - H%*%as.matrix(Yi))^2)/(N-2)\n\n[1] 24.13302\n\n# doing it the matrix way \ne.dot &lt;- Yi - H%*%as.matrix(Yi)\n(t(e.dot)%*%e.dot)/(N-2) # just a vector of sigmas\n\n         [,1]\n[1,] 24.13302\n\n# looking at the variance from the linear model fit \nas.numeric(summary(lm.fit)[6])^2 \n\n[1] 24.13302\n\n\nWe see both the sum and matrix method give the same thing and they match the linear model output from R! Note that we square the value from the linear model fit since that is just \\(\\sigma\\) which is the standard deviation (i.e. the square root of the variance).\n\n\nBack to hypothesis testing\nBut remember our goal: to see if there is any linear relationship between our independent variable \\(X_i\\) and our dependent variable \\(Y_i\\).\nWe can formulate this into a hypothesis test. The general steps are\n\nCreate the null and alternative hypotheses.\nMake an assumption about the processes that give rise to your data and choose a test statistic that will summarize the data that addresses this assumption. Note that this test statistic will come from a specific distribution. You can’t mix and match test statistics with distributions. Test statistics, based on how they are formulated, will have certain distributions that they come from.\nDecide a threshold or significance level, \\(\\alpha\\) such that if the probability of getting the test statistic that we have given our null distribution is as extreme or more, we reject the null hypothesis. This also known as the p-value.\n\nSo what is our null and alternative hypothesis? We care if there is any linear relationship between our independent and dependent variables, i.e. if our slope is not 0. We can formulate this as\n\\[\n\\begin{align}\nH_0 : \\beta_0 = 0\n\\\\ H_A : \\beta_1 \\neq 0\n\\end{align}\n\\]\nHow do we come up with our test statistic? The idea is that this test statistic will come from a distribution such that we will be able to evaluate if the probability of getting this value from this sample of data was extreme enough to either reject or not reject our null hypothesis, \\(H_A\\).\nA short digression on the frequentist framework. The data that one has, \\(Y_i\\) is viewed as a sample of a larger population. Because of this, we will never get the true, fixed values of \\(\\beta_0\\) and \\(\\beta_1\\). We can only do our best to approximate their values, \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\). We can make the argument, that if were to sample our population a bunch of times (i.e. get a bunch of \\(Y_i\\)’s, we would then be able to construct a sampling distribution of different \\((\\hat{\\beta_0}, \\hat{\\beta_1})\\) pairings. Obviously this would take a lot of time and usually we don’t necessarily have a cost or time-effective way to come up with a bunch of \\(Y_i\\) samples. This is where the Central Limit Theorem comes in!\nThe Central Limit Theorem states that if one was to construct a sampling distribution of means from different samples of data, as the number of sample means gets larger, the sampling distribution approaches a normal distribution.\nThinking about this, we can state that \\(\\beta_1\\) will be normally distributed. Note that we also will have a sampling distribution for \\(\\sigma^2\\), which is \\(\\chi^2\\) distributed.\n\n\nQuick general facts about some test-statistic distributions\nNote that there are test statistics that come from different distributions, but in general, we have the t-distribution, F-distribution, and \\(\\chi^2\\) distribution.\nThe t-distribution with \\(\\nu\\) degrees of freedom is defined as the distribution of a random variable \\(T\\), where \\[\nT = \\frac{Z}{\\sqrt{V/\\nu}} = Z\\sqrt{\\frac{\\nu}{V}}\n\\] Where \\(Z\\) is standard normally distributed, \\(V\\) has a \\(\\chi^2\\)-distribution with \\(\\nu\\) degrees of freedom and \\(Z\\) and \\(V\\) are independent. The main usage of the t-distribution in this context is by testing the difference between two means and whether that difference is “significant” given a pre-determined threshold \\(\\alpha\\) or not.\nThe F-distribution is defined as \\[\nX = \\frac{U_1/d_1}{U_2/d_2}\n\\] where \\(U_1\\) and \\(U_2\\) are independent random variables with chi-square distributions with respective degrees of freedom \\(d_1\\) and \\(d_2\\).\nThe \\(\\chi^2\\) distribution is defined as \\[\nX = \\Sigma_{i=1}^k Z_i^2\n\\] where \\(Z_i\\) are independent, standard normal random variables. This distribution is often used in goodness of fit tests and likelihood-ratio tests for nested models, among other things.\n\n\nSo what is our test statistic then?\nWe are testing for the probability that there is some sort of relationship between our \\(Y_i\\) and \\(X_i\\). We have assumed a null hypothesis, \\(H_A\\) that states that \\(\\beta_1 = 0\\). This our assumed truth. So, if we take our sample \\(\\hat{\\beta_1}\\), and we standardize it, we will get a value that tells us how far we are from our true \\(\\beta_1\\) (and note that this will be t-distributed!)\n\\[\nT = \\frac{\\hat{\\beta_1} - \\beta_1}{SE(\\hat{\\beta_1})}\n\\] And because in this hypothesis test, we are testing against \\(\\beta_1 = 0\\), this will simplify to\n\\[\nT = \\frac{\\hat{\\beta_1}}{SE(\\hat{\\beta_1})}\n\\]\n\n\nFinding the standard errors\nSo now we have the solution to the least squares equations. We have a vector of \\(\\hat{\\beta}\\)’s, as well as an estimate for the variance, \\(s^2\\). Now we need to find the standard error for the \\(\\hat{\\beta}\\)’s so we can create our hypothesis test test-statistic of \\(\\beta_1 / SE(\\beta_1)\\) which we now know will be t-distributed.\nWe recall from up way above that \\(\\vec{\\beta} = (X^TX)^{-1}X^TY\\). Also note this neat trick that given a matrix \\(A\\) and vector \\(x\\), \\(Var(Ax) = A*Var(x)*A^T\\) and that given a symmetric matrix \\(A\\), \\(A = A^T\\) and \\(A^{-1}\\) is also a symmetric matrix. In our case, we know that \\(X^TX\\) is a symmetric matrix.\n\\[\n\\begin{align}\nVar(\\beta) &= Var((X^TX)^{-1}X^T Y)\n\\\\\\\\ &= (X^TX)^{-1}X^T * Var(Y) * ((X^TX)^{-1}X^T)^T\n\\\\\\\\ &= (X^TX)^{-1}X^T * \\sigma^2 I * ((X^TX)^{-1}X^T)^T\n\\\\\\\\ &= \\sigma^2 I * (X^TX)^{-1}X^T * ((X^TX)^{-1}X^T)^T\n\\\\\\\\ &= \\sigma^2 I * (X^TX)^{-1}X^T * (X^T)^T ((X^TX)^{-1})^T\n\\\\\\\\ &= \\sigma^2 I * (X^TX)^{-1}X^T * X ((X^TX)^{-1})^T\n\\\\\\\\ &= \\sigma^2 I * (X^TX)^{-1} * (X^T X) * ((X^TX)^{-1})^T\n\\\\\\\\ &= \\sigma^2 I * ((X^TX)^{-1})^T\n\\\\\\\\ &= \\sigma^2 I * (X^TX)^{-1}\n\\end{align}\n\\] Now we only have estimates for some of these values, so in practice, we have that \\[\nVar(\\hat{\\beta}) = s^2 I * (X^TX)^{-1}\n\\] Let’s check this out in R.\n\n# getting our sigma estimate as before \nsigma.hat &lt;- sum((Yi - H%*%as.matrix(Yi))^2)/(N-2)\n\n# turning it into an diagonal matrix \nsigma.hat &lt;- matrix(c(sigma.hat,0,0,sigma.hat), nrow = 2) # sigma*Identity matrix\n\n# the matrix of residuals should be \nsigma.hat %*% solve(t(X.matrix)%*%X.matrix)\n\n            int         Xi\n[1,]  1.1381275 -0.1750349\n[2,] -0.1750349  0.0341629\n\n# checking with the linear model fit\nvcov(lm.fit) # compare with above \n\n            (Intercept)         Xi\n(Intercept)   1.1381275 -0.1750349\nXi           -0.1750349  0.0341629\n\n\nWe can also then check for the standard errors since the square root of the variance estimate should be the standard errors.\n\n# estimated standard error for beta0\nsqrt( (sigma.hat %*% solve(t(X.matrix)%*%X.matrix))[1,1] )\n\n     int \n1.066831 \n\n# estimated standard error for beta1\nsqrt( (sigma.hat %*% solve(t(X.matrix)%*%X.matrix))[2,2] )\n\n       Xi \n0.1848321 \n\n# SE from the linear model fit for beta0\nsqrt(vcov(lm.fit)[1,1]) \n\n[1] 1.066831\n\n# SE from the linear model fit for beta0\nsqrt(vcov(lm.fit)[2,2]) \n\n[1] 0.1848321\n\n\nWe see that they match! Neat!\n\n\nCalculating our test statistic\nNow that we have our \\(\\hat{\\beta_1}\\) and our \\(SE(\\hat{\\beta_1})\\), we can now construct our test statistic and evaluate it.\n\n# getting our derived variance covariance matrix \nvarcovar.matrix &lt;- sigma.hat %*% solve(t(X.matrix)%*%X.matrix)\n\n# calculating our test statistic\nbeta1_hat &lt;- betas[2,1]\nbeta1_SE &lt;- sqrt(varcovar.matrix[2,2])\ntest.statistic &lt;- beta1_hat/beta1_SE\n\nHow do we evaluate this test statistic? We know that it comes from a t-distribution and we want evaluate the probability that we would get a value as extreme or more.\n\n\nA small digression into probability distributions\nA probability distribution is a function that gives the probabilities of an event happening where \\(P(A \\leq x \\leq B) = \\int_a^b f(x;\\theta) dx\\) (for a continuous distribution), where \\(\\theta\\) are the parameters that describe the probability distribution. In R, there are four different types of functions for probability distributions. These functions start with either d, p, q, or r.\n\nddistr: d is for density and returns the value of \\(f(y;\\theta)\\), i.e. the value of the probability density function (continuous distributions) or probability mass function (discrete distributions)\npdistr: p is for probability and returns a value of \\(F(y;\\theta)\\), the cumulative distribution function.\nqdistr: q is for quantile and returns a value from the inverse of \\(F(y; \\theta)\\) and is also known as the quantile function.\nrdistr: r is for random and generates a random value from the given distribution.\n\n\nplot(seq(-3, 3, 0.01), dnorm(seq(-3, 3, 0.01), 0, 1), cex = 0.5)\n# plotting the area under the curve for what pnorm(2) is\npolygon(c(seq(-3, qnorm(0.977), 0.01), rev(seq(-3, qnorm(0.977), 0.01))),\n        c(rep(0, length(seq(-3, qnorm(0.977), 0.01))), rev(dnorm(seq(-3, qnorm(0.977), 0.01), 0, 1))),\n        col = 'gray', border = NA)\n# plotting the value of N(0, 1) at P(x = 2)\npoints(2, dnorm(2, 0, 1), pch = 19, col = 'green') \n# plotting that given the value of x such that the cumulative probability is 0.977\npoints(qnorm(0.977, 0, 1), 0, pch = 19, col = 'blue')\n# plotting 50 random variables drawn from a N(0, 1)\npoints(rnorm(50, 0, 1), rep(0, 50), pch = 20, col = 'red')\n\n\n\n\n\n\n\n\n\n\nEvaluating the test statistic\nIn our case, we want to find what the probability of getting a value that is equal to or more extreme than the test statistic value given that the null hypothesis is true. This is know as our p-value.\nLet’s look at this in R.\n\n# # we should get the same estimates of the t-value and p-value from the lm\np.val &lt;- 2*(1-pt(abs(test.statistic), N-2)) # or 2*pt(beta1_hat/beta1_SE, N-2), so we do abs() to account\np.val\n\n          Xi \n8.982047e-05 \n\n# checking with the linear model fit\nsummary(lm.fit)[4]\n\n$coefficients\n             Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 2.5305191  1.0668306 2.371997 1.964604e-02\nXi          0.7552312  0.1848321 4.086039 8.982047e-05\n\n# let's plot this!\nplot(seq(-8, 8, 0.01), dt(seq(-8, 8, 0.01), N-2), cex = 0.2)\n# plotting that given the value of x such that the cumulative probability is our p-value\npoints(qt(p.val, N-2), 0, pch = 19, col = 'blue')\n# we also care about the other extreme end\npoints(1-qt(p.val, N-2), 0, pch = 19, col = 'red')\n\n\n\n\n\n\n\n\nAn easy question to follow this, is why do we multiply by two above? In this formulation of our hypothesis test, we have a two-tailed test. We are assuming that \\(\\beta_1 = 0\\) and seeing if it \\(\\neq 0\\) as opposed to just \\(&gt;\\) or \\(&lt;\\). Therefore, we have to consider the probability that it is far greater than 0 or far less than 0. In the graph above, we care about the area under the curves to the left of the blue point and to the right of the red point.\n\n\nHow good is this value to reject our null hypothesis? Is there a set of values?\nIdeally before going through all of this, you will have chosen a threshold value, \\(\\alpha\\). This is the probability you are willing to reject the null hypothesis at. If your p-value lies below this threshold, then we would reject the null hypothesis. If it is above, we simply do not reject the null hypothesis. This does not mean the null hypothesis is true, it simply means you do not have a set of data / model that is able to reject the null.\nWe can also construct an interval such that there is a \\((1-\\alpha)\\)% probability that the interval would contain the true parameter. This is called the confidence interval for the value.\n$$\n$$\nHow are test statistics and confidence intervals related?\n\n\nLooking at additional information (\\(R^2\\) and the F-statistic)\n\\(R^2\\)\n\n# SST &lt;- sum((Y1 - mean(Y1))^2) # sum of residuals from the mean, df = n-1\n# SSE &lt;- sum((Y1 - H%*%as.matrix(Y1))^2) # sum of residuals from best fit line, df = n-p\n# SSR &lt;- SST - SSE # the difference, df = p-1. based on pythagorean theorem  \n# \n# # this should match the R-squared\n# SSR/SST # also can be written as 1 - SSE / SST\n# \n# # NEED TO FIGURE OUT ADJUSTED R^2\n# # Adjusted R-squared:  0.3764 \n# # we can adjust using the degrees of freedom, SSR/(p-1) / SST/(n-1)\n# SSR/(SST/(N-1))\n# (SSR/SST)*(N-1)\n\n\n# this should match the f-stat p-value from the lm \n# 2*(1-pf(abs(SSR/(SSE/(N-2))), 1, N-2)) # like the t-dist, SSR/(SSE/(N-2)) is the f-statistic\n\n\n\nAdditional resources\n\nOn matrix properties: https://www.ucl.ac.uk/~ucahhwi/MATH6502/handout9.pdf\nMore on the hat matrix: https://pj.freefaculty.org/guides/stat/Regression/RegressionDiagnostics/OlsHatMatrix.pdf\nMore on simple linear regression with matrices: https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/13/lecture-13.pdf\nOn a geometric interpretation: https://bookdown.org/ts_robinson1994/10EconometricTheorems/linear_projection.html\nA nice video on explanation of variance: https://www.youtube.com/watch?v=x0rmUXWtSS8\nmore in depth video of the least square estimators: https://www.youtube.com/watch?v=D2ns91Vcs8U"
  },
  {
    "objectID": "stat-blog/in_progress/distributions.html",
    "href": "stat-blog/in_progress/distributions.html",
    "title": "Distributions",
    "section": "",
    "text": "Learning statistics is a language, and for me, learning the distributions is like learning the grammar. Once you get the reasons for why the distributions are what they are (and why the have the form they do), it simplifies the mystery of why we use certain distributions for certain problems (and if you do Bayesian inference, it can help inform you of what priors you want to use).\n\nDiscrete\nBernoulli: What is the probability I get one success with only one trial?\nBinomial: Let’s expand on the Bernoulli distribution. What is the probability I get k successes in n trials?\nGeometric: Let’s also expand on the Bernoulli distribution, except now I only care about the first success in many trials until I stop. What is the probability a success occurs on the kth trial?\nNegative Binomial: Let’s expand on the Geometric distribution now because I want to end on a certain number of successes. What is the probability r number of successes finishes accruing on the kth trial? Note that the Poisson distribution is a special case of this.\nHypergeometric: What if I have a finite number of successes to find. What is the probability that I find k successes out of r total successes, where each time I remove a success I cannot sample it again (i.e. no replacement).\nPoisson: Let’s push the Binomial distribution to its limit. What is the probability of k events happening in a given interval of time given a rate \\(\\lambda\\)? Note that for really large n and really small p, the Poisson frequency function can be used instead of the binomial.\n\n\nContinuous\nUniform: What if there is an equal chance for each probability to arise? Note that this is a special case of the Beta distribution.\nExponential: Let’s expand on the Poisson. What is the probability that another event doesn’t occur in a given time interval? Note that this is a special case of the gamma distribution.\nGamma: shape rate, used for survival times along with the Weibull distribution\nNormal: Useful for modeling random error or data that is centered around a mean.\nBeta: Useful for modeling random variables that are restricted to the interval [0, 1].\nA copula is a joint cumulative distribution function of random variables that have uniform marginal distributions. This construction points out that from the ingredients of two marginal distributions and any copula, a joint distribution with those marginals can be constructed. It is thus clear that the marginal distributions do not determine the joint distribution. Remember that if y = \\(F_x(X)\\), then y is a uniform distribution.\nconvolutions: given joint frequency function p(x,y) and x and y are indep. if we have Z + X+Y, we can rearrange our joint distribution to get the cdf and then the pdf of Z.\nUnder the assumptions just stated, the joint density of U and V is fUV(u,v)= fXY(h1(u,v),h2(u,v))|\\(J^{-1}\\)(h1(u,v),h2(u,v))| for (u,v) such that u = g1(x,y) and v = g2(x,y) for some (x,y) and 0 elsewhere.\npapers: Perry: Marti Anderson copulas, Perry 2014."
  },
  {
    "objectID": "stat-blog_wip/distributions.html",
    "href": "stat-blog_wip/distributions.html",
    "title": "Distributions",
    "section": "",
    "text": "Learning statistics is a language, and for me, learning the distributions is like learning the grammar. Once you get the reasons for why the distributions are what they are (and why the have the form they do), it simplifies the mystery of why we use certain distributions for certain problems (and if you do Bayesian inference, it can help inform you of what priors you want to use).\n\nDiscrete\nBernoulli: What is the probability I get one success with only one trial?\nBinomial: Let’s expand on the Bernoulli distribution. What is the probability I get k successes in n trials?\nGeometric: Let’s also expand on the Bernoulli distribution, except now I only care about the first success in many trials until I stop. What is the probability a success occurs on the kth trial?\nNegative Binomial: Let’s expand on the Geometric distribution now because I want to end on a certain number of successes. What is the probability r number of successes finishes accruing on the kth trial? Note that the Poisson distribution is a special case of this.\nHypergeometric: What if I have a finite number of successes to find. What is the probability that I find k successes out of r total successes, where each time I remove a success I cannot sample it again (i.e. no replacement).\nPoisson: Let’s push the Binomial distribution to its limit. What is the probability of k events happening in a given interval of time given a rate \\(\\lambda\\)? Note that for really large n and really small p, the Poisson frequency function can be used instead of the binomial.\n\n\nContinuous\nUniform: What if there is an equal chance for each probability to arise? Note that this is a special case of the Beta distribution.\nExponential: Let’s expand on the Poisson. What is the probability that another event doesn’t occur in a given time interval? Note that this is a special case of the gamma distribution.\nGamma: shape rate, used for survival times along with the Weibull distribution\nNormal: Useful for modeling random error or data that is centered around a mean.\nBeta: Useful for modeling random variables that are restricted to the interval [0, 1].\nA copula is a joint cumulative distribution function of random variables that have uniform marginal distributions. This construction points out that from the ingredients of two marginal distributions and any copula, a joint distribution with those marginals can be constructed. It is thus clear that the marginal distributions do not determine the joint distribution. Remember that if y = \\(F_x(X)\\), then y is a uniform distribution.\nconvolutions: given joint frequency function p(x,y) and x and y are indep. if we have Z + X+Y, we can rearrange our joint distribution to get the cdf and then the pdf of Z.\nUnder the assumptions just stated, the joint density of U and V is fUV(u,v)= fXY(h1(u,v),h2(u,v))|\\(J^{-1}\\)(h1(u,v),h2(u,v))| for (u,v) such that u = g1(x,y) and v = g2(x,y) for some (x,y) and 0 elsewhere.\npapers: Perry: Marti Anderson copulas, Perry 2014."
  },
  {
    "objectID": "stat-blog_wip/hypo-test-lin-reg.html",
    "href": "stat-blog_wip/hypo-test-lin-reg.html",
    "title": "Simple Linear Regression and Hypothesis Testing",
    "section": "",
    "text": "Model assumptions.\nConfidence intervals, p-values, linear least squares.\nSimulations\ntest"
  }
]